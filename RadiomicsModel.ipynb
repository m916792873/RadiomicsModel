{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaries 1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import radiomics \n",
    "import six,numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import pylab as pl\n",
    "import scipy, importlib, pprint, matplotlib.pyplot as plt, warnings\n",
    "import glmnet_python\n",
    "import imblearn\n",
    "import math\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries 2\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, normalize\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, VarianceThreshold, mutual_info_classif, SelectFromModel, SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression, RFE, RFECV\n",
    "from sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV, ridge_regression, SGDClassifier, RidgeClassifier, PassiveAggressiveClassifier, OrthogonalMatchingPursuit, MultiTaskLasso, LassoLars, Lars, RidgeCV, ElasticNet, ElasticNetCV, LogisticRegression, LassoLarsCV, LassoCV, LinearRegression, Ridge, Lasso, lasso_path, enet_path\n",
    "from sklearn.model_selection import LeaveOneOut, cross_validate, cross_val_predict, train_test_split, StratifiedKFold, cross_val_score, GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.svm import LinearSVC, SVC, SVR, NuSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, roc_curve, auc, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier, OutputCodeClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils import resample\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from radiomics import featureextractor,imageoperations\n",
    "from pandas import Series, DataFrame \n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from glmnet import glmnet\n",
    "from glmnet import glmnet; from glmnetPlot import glmnetPlot\n",
    "from glmnetPrint import glmnetPrint; from glmnetCoef import glmnetCoef; from glmnetPredict import glmnetPredict\n",
    "from cvglmnet import cvglmnet; from cvglmnetCoef import cvglmnetCoef\n",
    "from cvglmnetPlot import cvglmnetPlot; from cvglmnetPredict import cvglmnetPredict\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, ClusterCentroids\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv files \n",
    "\n",
    "file = '/Users/hwang/Documents/research/1_MM/pyradiomics/scikit_radiomics_T1norm_noshape_MWU.csv'\n",
    "data = pd.read_csv(file)\n",
    "target = data['Class']\n",
    "target = target.astype(np.float64)\n",
    "feature = data.loc[:,data.columns != 'Class']\n",
    "reader = csv.DictReader(open(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrleation analysis to eliminate one of the highly correlated features \n",
    "\n",
    "feature = pd.DataFrame(feature)\n",
    "\n",
    "imp_feat = feature\n",
    "\n",
    "f = plt.subplots(figsize=(30,30))\n",
    "corr_mat = imp_feat.corr().abs()\n",
    "ax = sn.heatmap(corr_mat,annot=True)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()\n",
    "print(imp_feat.shape)\n",
    "\n",
    "col_corr = set() # Set of all the names of deleted columns\n",
    "threshold = 0.9\n",
    "corr_matrix = imp_feat.corr()\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n",
    "            colname = corr_matrix.columns[i] # getting the name of column\n",
    "            col_corr.add(colname)\n",
    "            if colname in imp_feat.columns:\n",
    "                del imp_feat[colname] # deleting the column from the dataset\n",
    "                \n",
    "feature = imp_feat\n",
    "print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test sets split \n",
    "\n",
    "name = reader.fieldnames[1:]\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(feature,target,test_size=0.25,random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "encoder = LabelEncoder()\n",
    "feature_train = scaler.fit_transform(feature_train)\n",
    "feature_test = scaler.fit_transform(feature_test)\n",
    "encoder.fit(target_train)\n",
    "encoder.fit(target_test)\n",
    "target_train = encoder.fit_transform(target_train)\n",
    "target_test = encoder.fit_transform(target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For balancing unbalanced class (plot_2d_space)\n",
    "\n",
    "def plot_2d_space(X, y, label='Classes'):  \n",
    "    \n",
    "    colors = ['#1F77B4', '#FF7F0E']\n",
    "    markers = ['o', 's']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(\n",
    "            X[y==l, 0],\n",
    "            X[y==l, 1],\n",
    "            c=c, label=l, marker=m\n",
    "        )\n",
    "    plt.title(label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For balancing unbalanced class (code)\n",
    "\n",
    "# ros = RandomOverSampler()\n",
    "ros = SMOTE(ratio='minority')\n",
    "# ros = RandomUnderSampler(return_indices=True)\n",
    "# ros = ClusterCentroids(ratio={0:10})\n",
    "# ros = TomekLinks(return_indices=True, ratio='majority')\n",
    "\n",
    "feature_train_ros, target_train_ros = ros.fit_sample(feature_train, target_train)\n",
    "print(feature_train_ros.shape[0] - feature_train.shape[0], 'new random picked points')\n",
    "# print('Removed indexes:', id_ros)\n",
    "\n",
    "feature_train = feature_train_ros\n",
    "target_train = target_train_ros\n",
    "\n",
    "plot_2d_space(feature_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO regularization: to determine alpha and important features \n",
    "\n",
    "lassocv = LassoCV(eps = 0.001, n_alphas = 100, alphas=None, cv=5, max_iter=10000000, normalize=True)\n",
    "lassocv.fit(feature_train,target_train)\n",
    "\n",
    "print(\"Alpha =\", lassocv.alpha_)\n",
    "train_model_score = lassocv.score(feature_train, target_train)\n",
    "test_model_score = lassocv.score(feature_test, target_test)\n",
    "coff_used = np.sum(lassocv.coef_ != 0)\n",
    "\n",
    "print(\"mse =\",mean_squared_error(target_test, lassocv.predict(feature_test)))\n",
    "print(\"Number of features used:\", coff_used)\n",
    "\n",
    "feature_train_inframe = pd.DataFrame(feature_train)\n",
    "features = pd.Series(lassocv.coef_, index=feature_train_inframe.columns)\n",
    "\n",
    "for i,v in enumerate(np.nonzero([features])):\n",
    "    print(v)\n",
    "    \n",
    "x = v.astype(int)\n",
    "imp_feature_name = list()\n",
    "imp_feature_val = list()\n",
    "imp_feature_ind = list()\n",
    "for i in range(x.shape[0]):\n",
    "    ind = features.index[x[i]]\n",
    "    feat = name[x[i]]\n",
    "    val = features.values[x[i]]\n",
    "    print(ind, feat, val)\n",
    "    imp_feature_name.append(feat)\n",
    "    imp_feature_val.append(val)\n",
    "    imp_feature_ind.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO regularization: for mean square error plot with respect to alpha (lambda)\n",
    "\n",
    "model = lassocv\n",
    "\n",
    "m_log_alphas = -np.log10(model.alphas_)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(m_log_alphas, model.mse_path_, ':')\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n",
    "         label='Average across the folds', linewidth=2)\n",
    "plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha: CV estimate')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('-Log(Lambda)')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO regularization: correlation matrix among the selected features from lassocv\n",
    "\n",
    "header = []\n",
    "col = []\n",
    "for i in range(x.shape[0]):\n",
    "    # col.append(feature1.values[x[i]])\n",
    "    col.append(feature.iloc[:,x[i]])\n",
    "    header.append(name[x[i]])\n",
    "\n",
    "col = np.array(col)\n",
    "col = np.transpose(col)\n",
    "frame = pd.DataFrame(col, columns = header)\n",
    "\n",
    "ax = sn.heatmap(frame.corr(), annot=True)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO regularization: list important features selected from LASSO regularization  \n",
    "\n",
    "model = lassocv\n",
    "sfm = SelectFromModel(model, threshold=0.00001)\n",
    "sfm.fit(feature_train,target_train)\n",
    "\n",
    "imp_feature_train = sfm.transform(feature_train)\n",
    "imp_feature_test = sfm.transform(feature_test)\n",
    "\n",
    "feature_train = imp_feature_train\n",
    "feature_test = imp_feature_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO regularization: classification and cross validation using LASSO linear regression \n",
    "\n",
    "cv = KFold(5, shuffle=True, random_state=3)\n",
    "# cv = LeaveOneOut()\n",
    "# scores_loo = cross_val_score(model, feature_train, target_train, cv=cv) \n",
    "cvscore = cross_val_score(model, feature_train, target_train, scoring=\"r2\", cv=cv)\n",
    "# cvresult = cross_validate(lassocv, feature_train, target_train, cv=cv, return_train_score=True)\n",
    "cvprediction = cross_val_predict(model, feature_train, target_train, cv=cv)\n",
    "predicted = cvprediction.round()\n",
    "\n",
    "for i in range(predicted.shape[0]):\n",
    "    if predicted[i] > 1.0: \n",
    "        predicted[i] = 1\n",
    "    if predicted[i] <= -1.0:\n",
    "        predicted[i] = 0\n",
    "            \n",
    "accuracy = accuracy_score(target_train,abs(predicted))\n",
    "\n",
    "tn_train, fp_train, fn_train, tp_train = confusion_matrix(target_train, abs(predicted)).ravel()\n",
    "sensitivity_train = tp_train / (tp_train + fn_train)\n",
    "specificity_train = tn_train / (tn_train + fp_train)\n",
    "auctraining = roc_auc_score(target_train, abs(cvprediction))\n",
    "\n",
    "# print(\"Validation score after 5-fold cross validation:\", cvresult['test_score'].mean())\n",
    "print(\"Classification accuracy on training set after 5-fold cross validation: {:2f}\".format(accuracy))\n",
    "print(\"Sensitivity on training set: {:2f}\".format(sensitivity_train))\n",
    "print(\"Specificity on training set: {:2f}\".format(specificity_train))\n",
    "print(\"Training score after 3-fold cross validation:\", cvscore.mean())\n",
    "\n",
    "target_test_pred = model.predict(feature_test)\n",
    "asall_test = accuracy_score(target_test, target_test_pred.round())\n",
    "auctest = roc_auc_score(target_test, target_test_pred)\n",
    "predicted_test = target_test_pred.round()\n",
    "\n",
    "for i in range(predicted_test.shape[0]):\n",
    "    if predicted_test[i] > 1.0: \n",
    "        predicted_test[i] = 1\n",
    "    if predicted_test[i] <= -1.0:\n",
    "        predicted_test[i] = 0\n",
    "\n",
    "tn_test, fp_test, fn_test, tp_test = confusion_matrix(target_test, abs(predicted_test)).ravel()\n",
    "sensitivity_test = tp_test / (tp_test + fn_test)\n",
    "specificity_test = tn_test / (tn_test + fp_test)\n",
    "\n",
    "\n",
    "print(\"Classification accuracy on test set: {:2f}\".format(asall_test))\n",
    "print(\"Sensitivity on test set: {:2f}\".format(sensitivity_test))\n",
    "print(\"Specificity on test set: {:2f}\".format(specificity_test))\n",
    "print(\"Test score:\", test_model_score)\n",
    "print(\"Area under the ROC curve (training): {:2f}\".format(auctraining))\n",
    "print(\"Area under the ROC curve (test): {:2f}\".format(auctest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prinpal Component Analysis (PCA)\n",
    "\n",
    "pca = PCA(0.95)\n",
    "pca.fit(feature_train)\n",
    "pca.fit(feature_test)\n",
    "imp_feature_train = pca.transform(feature_train)\n",
    "imp_feature_test = pca.transform(feature_test)\n",
    "\n",
    "n_pcs = pca.components_.shape[0]\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "most_important_names = [name[most_important[i]] for i in range(n_pcs)]\n",
    "variance_ratios = [pca.explained_variance_ratio_[i] for i in range(n_pcs)]\n",
    "# most_important_ind = [imp_feature_ind[most_important[i]] for i in range(n_pcs)]\n",
    "dic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\n",
    "df = pd.DataFrame(dic.items())\n",
    "df['variance ratio'] = variance_ratios\n",
    "print(df)\n",
    "\n",
    "feature_train = imp_feature_train\n",
    "feature_test = imp_feature_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prinpal Component Analysis (PCA): correlation matrix after PCA\n",
    "\n",
    "header = []\n",
    "col = []\n",
    "for i in range(pca.components_.shape[0]):\n",
    "    col.append(feature.iloc[:,most_important[i]])\n",
    "    header.append(name[most_important[i]])\n",
    "\n",
    "col = np.array(col)\n",
    "col = np.transpose(col)\n",
    "frame = pd.DataFrame(col, columns = header)\n",
    "\n",
    "ax = sn.heatmap(frame.corr(), annot=True)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model construction using selected features: LR \n",
    "\n",
    "model = LogisticRegressionCV(cv=5, random_state=1, Cs=1)\n",
    "model.fit(feature_train,target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model construction using selected features: RF \n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=10)\n",
    "model.fit(feature_train,target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF: determine parameters\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 0, stop = 200, num = 11)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(0, 300, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [1,2,4]\n",
    "min_samples_leaf = [1,2,4]\n",
    "bootstrap = [True]\n",
    "oob_score = [True]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'oob_score': oob_score}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF: use random grid to search for best hyperparameters\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=10, n_jobs = 1)\n",
    "\n",
    "rf_random.fit(feature_train,target_train)\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, feature_test,target_test):\n",
    "    predictions = model.predict(feature_test)\n",
    "    errors = abs(predictions - target_test)\n",
    "    accuracy = (len(errors)-np.count_nonzero(errors))/(len(errors))\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF: \n",
    "\n",
    "pred_train = model.predict(feature_train)\n",
    "errors = abs(pred_train - target_train)\n",
    "accuracy = (len(errors)-np.count_nonzero(errors))/(len(errors))\n",
    "print('Model Performance')\n",
    "print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "print('Accuracy = {:0.2f}%.'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF: evaluate performance of random grid \n",
    "\n",
    "base_model = RandomForestClassifier(n_estimators = 100, random_state = 10)\n",
    "base_model.fit(feature_train,target_train)\n",
    "base_accuracy = evaluate(base_model, feature_test, target_test)\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, feature_test, target_test)\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF: Create a parameter grid based on the results from random search \n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [90],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [1,2],\n",
    "    'min_samples_split': [2,4],\n",
    "    'n_estimators': [180],\n",
    "    'oob_score': [True]}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 5, n_jobs = 1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF: Fit the grid search to the data\n",
    "\n",
    "grid_search.fit(feature_train,target_train)\n",
    "grid_search.best_params_\n",
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, feature_test, target_test)\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF: cross validation and result display using the model other than lasso \n",
    "\n",
    "model = best_grid\n",
    "\n",
    "cv = 5\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(feature_train)\n",
    "scv = []\n",
    "for train_idx, test_idx in loo.split(feature_train):\n",
    "    model.fit(feature_train[train_idx,:], target_train[train_idx])\n",
    "    s = model.score(feature_train[test_idx,:], target_train[test_idx])\n",
    "    scv.append(s)\n",
    "scores_loo = cross_val_score(model, feature_train, target_train, cv=cv) \n",
    "\n",
    "# kfold = KFold(3, shuffle=True, random_state=5)\n",
    "# scores_kfold = cross_val_score(model, feature_train, target_train, cv=cv) \n",
    "\n",
    "cvprediction = cross_val_predict(model, feature_train, target_train, cv=cv, method = 'predict_proba')\n",
    "predicted = abs(cvprediction[:,1])\n",
    "\n",
    "for i in range(predicted.shape[0]):\n",
    "    if predicted[i] > 1.0: \n",
    "        predicted[i] = 1\n",
    "    if predicted[i] <= -1.0:\n",
    "        predicted[i] = 0\n",
    "        \n",
    "accuracy = accuracy_score(target_train,predicted.round())\n",
    "\n",
    "tn_train, fp_train, fn_train, tp_train = confusion_matrix(target_train, abs(predicted.round())).ravel()\n",
    "sensitivity_train = tp_train / (tp_train + fn_train)\n",
    "specificity_train = tn_train / (tn_train + fp_train)\n",
    "auctraining = roc_auc_score(target_train, cvprediction[:,1])\n",
    "\n",
    "\n",
    "# results based on cross-validation \n",
    "print(\"Classification accuracy on training set after 3-fold cross validation: {:2f}\".format(accuracy))\n",
    "print(\"Sensitivity on training set: {:2f}\".format(sensitivity_train))\n",
    "print(\"Specificity on training set: {:2f}\".format(specificity_train))\n",
    "\n",
    "# results without cross-validation \n",
    "target_train_pred = model.predict_proba(feature_train)\n",
    "target_test_pred = model.predict_proba(feature_test)\n",
    "asall_test = accuracy_score(target_test, target_test_pred[:,1].round())\n",
    "auctest = roc_auc_score(target_test, target_test_pred[:,1])\n",
    "predicted_test = target_test_pred[:,1].round()\n",
    "\n",
    "for i in range(predicted_test.shape[0]):\n",
    "    if predicted_test[i] > 1.0: \n",
    "        predicted_test[i] = 1\n",
    "    if predicted_test[i] <= -1.0:\n",
    "        predicted_test[i] = 0\n",
    "\n",
    "tn_test, fp_test, fn_test, tp_test = confusion_matrix(target_test, abs(predicted_test)).ravel()\n",
    "sensitivity_test = tp_test / (tp_test + fn_test)\n",
    "specificity_test = tn_test / (tn_test + fp_test)\n",
    "\n",
    "\n",
    "print(\"Classification accuracy on test set: {:2f}\".format(asall_test))\n",
    "print(\"Sensitivity on test set: {:2f}\".format(sensitivity_test))\n",
    "print(\"Specificity on test set: {:2f}\".format(specificity_test))\n",
    "print(\"Area under the ROC curve (training): {:2f}\".format(auctraining))\n",
    "print(\"Area under the ROC curve (test): {:2f}\".format(auctest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF: display important features selected by Random Forest \n",
    "\n",
    "feature_imp = pd.Series(model.feature_importances_,index=feature.columns).sort_values(ascending=True)\n",
    "# feature_imp = pd.Series(model.feature_importances_,index=frame.columns).sort_values(ascending=True)\n",
    "sn.barplot(x = feature_imp, y = feature_imp.index)\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC: with 95% CI \n",
    "\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:http://localhost:8888/notebooks/Documents/pyradiomics-master/notebooks/radiomics_lassocv.ipynb\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "alpha = .95\n",
    "auc_train, auc_cov_train = delong_roc_variance(target_train,abs(cvprediction))\n",
    "auc_test, auc_cov_test = delong_roc_variance(target_test,abs(predicted_test))\n",
    "\n",
    "auc_std_train = np.sqrt(auc_cov_train)\n",
    "auc_std_test = np.sqrt(auc_cov_test)\n",
    "lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "ci_train = stats.norm.ppf(\n",
    "    lower_upper_q,\n",
    "    loc=auc_train,\n",
    "    scale=auc_std_train)\n",
    "\n",
    "ci_test = stats.norm.ppf(\n",
    "    lower_upper_q,\n",
    "    loc=auc_test,\n",
    "    scale=auc_std_test)\n",
    "\n",
    "ci_train[ci_train > 1] = 1\n",
    "ci_test[ci_test > 1] = 1\n",
    "\n",
    "print('AUC (train):', auc_train)\n",
    "print('AUC COV (train):', auc_cov_train)\n",
    "print('95% AUC CI (train):', ci_train)\n",
    "\n",
    "print('AUC (test):', auc_test)\n",
    "print('AUC COV (test):', auc_cov_test)\n",
    "print('95% AUC CI (test):', ci_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
